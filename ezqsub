#!/bin/env python3
# -*- coding: utf-8 -*-
# vim:fenc=utf-8 tabstop=4 expandtab shiftwidth=4 softtabstop=4
#
# Copyright Â© Mike Dacre <mike.dacre@gmail.com>
#
# Distributed under terms of the MIT license
"""
====================================================================================

          FILE: ezqsub (python 3)
        AUTHOR: Michael D Dacre, mike.dacre@gmail.com
  ORGANIZATION: Stanford University
       LICENSE: MIT License
       VERSION: 0.1
       CREATED: 2013-12-26 17:37
 Last modified: 2014-01-21 15:51

   DESCRIPTION: Take a file of scripts and submit it to the gordon cluster
                The file should be one line per job, the lines can be arbitrarily
                long and complex bash scripts, just use semi-colons instead of new-
                lines.

         USAGE: ezqsub script_file.txt or ezqsub < script_file.txt

====================================================================================
"""

import sys, os

debug = 0

# Get user name
user = os.environ['USER']

# Set the defaults
default_threads  = 16
default_commands = default_threads

default_address  = 'sua135'
default_queue    = 'normal'
default_walltime = '1:00:00'
default_params   = ''
default_modules  = 'python'
default_tmpdir   = ''.join(['/oasis/scratch/', user, '/temp_project/'])

def run_parallel(infile, threads=default_threads):
    """Take a file path to a file of commands and execute them in parallel.
       threads variable takes the number of threads to run at once

       This should not usually be run directly in this script, it is intended
       for node running.

       Returns a dictionary of results:
           command -> (output_code, stdout_and_stderr)

       If verbose, results of any failed command will also be logged
       to logfile (default STDERR)"""

    # We use the multi-threading pool and subprocess for this
    from subprocess import getstatusoutput as call
    from multiprocessing import Pool
    pool = Pool(processes=int(threads))

    # List to hold results
    threads = []

    # Open file, and for each line, split line into list by whitespace
    # and pass to subprocess via multiprocessing pool
    with open(infile) as file:
        for line in file:
            if not line.startswith('#'):
                command = [line.rstrip()]
                threads.append( (line.rstrip(), pool.apply_async(call, command)) )

    # Run the threads
    results = []
    for name, thread in threads:
        results.append( (name, thread.get()) )

    # Return results
    return results


def split_scripts(infile, prefix='job', commands=default_commands, tmpdir=default_tmpdir):
    """
    Take a file of scripts and split it into temp script files to be submitted
    to the cluster.
    The file should be one line per job, the lines can be arbitrarily
    long and complex bash scripts, just use semi-colons instead of new-
    lines.
    The commands variable specifies how many lines to create per file, or in
    other words, how many jobs to run per node.
    NOTE: If the number of commands is less than the number of processes, the
    extras will be run in series.

    The first argument must be an open file handle
    """
    from tempfile import mkstemp
    import os

    outfile_name    = ''
    filtered_lines  = 0
    files           = []
    fd              = ''

    for line in infile:
        if not filtered_lines:
            filtered_lines = int(commands)
            fd, outfile_name = mkstemp(dir=tmpdir, text=True, prefix=prefix)
            os.close(fd)
            files.append(os.path.realpath(outfile_name))

        with open(outfile_name, 'a') as outfile:
            print(line.rstrip(), file=outfile)

        filtered_lines = filtered_lines - 1

    return(files)

def submit_files(file_list, modules=default_modules, name='job', queue=default_queue, params=default_params, walltime=default_walltime, address=default_address, threads=default_threads):
    """ Take a list of files, and submit each file to qsub
        You can modify the queue with the queue variable or any other qsub
        parameters using the params variable """
    import subprocess
    from os import path

    my_loc = path.realpath(__file__)

    node_string = ''.join(['nodes=1:ppn=', str(threads), ':native'])

    pbs_job_nos = []

    job_number = 1
    for file in file_list:
        # Create the unique job name
        job_name = '_'.join([name, str(job_number)])
        job_number = job_number + 1

        # Create the virtual file for pbs
        template = "#!/bin/bash\n#PBS -S /bin/bash\n"
        template = ''.join([template, "#PBS -q ", queue, '\n#PBS -N ', job_name,
                            '\n#PBS -A ', address, '\n#PBS -l ', node_string,
                            '\n#PBS -l walltime=', walltime])
        if params:
            template = ''.join([template,'\n#PBS ', params])

        template = ''.join([template, '\n\ncd $PBS_O_WORKDIR\n\n'])
        for module in modules:
            template = ''.join([template, 'module load ', module, '\n'])
        template = ''.join([template, '\n', my_loc, ' -f ', file, ' -t ', str(threads), '\n'])

        if debug:
            print(template)

        # Craft the pbs qsub command
        pbs_command = ('qsub', '-q', queue, '-N', name, '-A', address, '-l', node_string)

        if debug:
            print(pbs_command)

        # Submit
        pbs_submit = subprocess.Popen(pbs_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE)
        pbs_submit.stdin.write(template.encode())
        pbs_submit.stdin.close()

        # Get job number
        pbs_job_nos.append(pbs_submit.stdout.read().decode().rstrip())
        pbs_submit.stdout.close()

    return(pbs_job_nos)

def cleanup_temp(tmpdir=default_tmpdir, prefix='job'):
    """Delete all files with the job name prefix in provided tempfile"""
    from os import listdir, remove

    # Get filelist from directory
    file_list = listdir(tmpdir)

    # Loop through file list, delete all that start with prefix
    for file in file_list:
        if file.startswith(prefix):
            remove('/'.join([tmpdir,file]))

def _get_args():
    """Command Line Argument Parsing"""
    import argparse

    parser = argparse.ArgumentParser(
                 description=__doc__,
                 formatter_class=argparse.RawDescriptionHelpFormatter)

    # Optional Files
    parser.add_argument('-i', '--infile', nargs='?', default='',
                        help="Input file, Default STDIN")
    parser.add_argument('-f', '--tmpfile', default='', help=argparse.SUPPRESS)

    # Debugging flag
    parser.add_argument('-x', '--debug', action='store_true',  help=argparse.SUPPRESS)

    # Optional Arguments
    parser.add_argument('-n', '--name', default='job',
            help="Job Name, will be prefix in qstat. Default: job")
    parser.add_argument('-t', '--threads', default=default_threads,
            help=''.join(["Over-ride number of threads per node, you should use this ",
                          "if you want less than ", str(default_threads), " to run at ",
                          "once on a single node. Note that you will still be billed for ",
                          "all ", str(default_threads), " cores. ",
                          "Default: ", str(default_threads)]) )
    parser.add_argument('--commands',
            help=''.join(["Over-ride number of commands sent to each node. This defaults ",
                          "to the same as '-t'. If you want less than ", str(default_threads),
                          "commands to run on a node, you can just set '-t'. If however, ",
                          "you want jobs to run in serial on a node, this can be a good option. \n",
                          "This option should be completely unnecessary most of the time\n",
                          "Default: ", str(default_commands)]) )
    parser.add_argument('-d', '--tmpdir', default=default_tmpdir,
            help=''.join(["Where to store job files - you must delete them manually, ",
                          "Default: ", default_tmpdir]) )
    parser.add_argument('-q', '--queue', default=default_queue,
            help=''.join(["Queue Choice, Default: ", default_queue]) )
    parser.add_argument('-m', '--modules', default=default_modules, nargs='+',
            help=''.join(["Choose modules to load, Default: ", default_modules]) )
    parser.add_argument('-w', '--walltime', default='',
            help=''.join(["Set walltime, use least possible, max=336:00:00, Default: ", default_modules]) )
    parser.add_argument('-p', '--params', default=default_params,
            help=''.join([ "qsub parameters. These are any additional qsub flags you ",
                           "wish to pass. Note that they should be enclosed in parentheses ",
                           "e.g. \"-l mem=32GB\", not just plain 'mem=32GB', ",
                           "If you don't include the flags, it won't work. Default: ",
                           default_params]) )
    parser.add_argument('-a', '--billing', default=default_address,
            help=''.join(["Choose the address to bill to, find this with ",
                          "show_account or on portal.xsedeq.org",
                          ", Default: ", default_address]) )
    parser.add_argument('--cleanup', action='store_true',
            help=''.join(["Cleanup your temporary directory. Please run this every now and ",
                          "then. IMPORTANT: DO NOT RUN IF YOU HAVE JOBS IN THE QUEUE!!!.",
                          "Default temp directory: ", default_tmpdir,
                          "Default prefix: 'job'. If you use the -n flag in any of your ",
                          "runs, use that same flag. e.g. if you ran with ",
                          "'ezqsub -n my_job -i my_job_script.txt', cleanup with ",
                          "'ezqsub -n my_job --cleanup'"]) )

    return parser

# Main function for direct running
def main():
    """Run directly"""
    # Get commandline arguments
    parser = _get_args()
    args = parser.parse_args()

    global debug

    if args.debug:
        debug = 1
    else:
        debug = 0

    # If '-f' is set, we are running on the nodes, so let's just do that
    if args.tmpfile:
        results = run_parallel(infile=args.tmpfile, threads=args.threads)

        # Loop through outputs and print
        for name, output in results:
            print(name, ':\n', 'Result code: ', output[0], '\nOutput:\n\n',
                    output[1], '\n\n\n', sep='')
        sys.exit(0)

    # If cleanup is chosen, do that first
    if args.cleanup:
        cleanup_temp(tmpdir=args.tmpdir, prefix=args.name)
        sys.exit(0)

    # Require walltime for command line run
    if not args.walltime:
        print(parser.print_help(), file=sys.stderr)
        print("Walltime required\nPlease prived a walltime. e.g.:\n",
              __file__, "-w 1:00:00 -i <script_file>", file=sys.stderr)
        sys.exit(1)

    # Turn modules into array - split on space or comma
    modules = []
    if isinstance(args.modules, str):
        for i in args.modules.split(','):
            modules.append(i)
    else:
        for i in args.modules:
            temps = i.split(',')
            for j in temps:
                modules.append(j)

    if 'python' not in modules:
        modules.insert(0, 'python')

    # Set number of commands per file
    if args.commands:
        commands = args.commands
    else:
        commands = args.threads

    # Split input file into temp files stored in /tmp
    if args.infile:
        with open(args.infile, 'r') as infile:
            tmpfiles = split_scripts(infile=infile, prefix=args.name, commands=commands, tmpdir=args.tmpdir)
    else:
        tmpfiles = split_scripts(infile=sys.stdin, prefix=args.name, commands=commands, tmpdir=args.tmpdir)

    # Loop through input files and submit via qsub
    results = submit_files(file_list=tmpfiles, modules=modules, name=args.name, queue=args.queue, walltime=args.walltime, params=args.params, address=args.billing, threads=args.threads)

    # Success
    print("PBS Jobs:")
    for result in results:
        print(result, file=sys.stderr)


# The end
if __name__ == '__main__':
    main()
