#!/oasis/projects/nsf/sua131/peanut/bin/python3
# -*- coding: utf-8 -*-
# vim:fenc=utf-8 tabstop=4 expandtab shiftwidth=4 softtabstop=4
#
# Copyright Â© Mike Dacre <mike.dacre@gmail.com>
#
# Distributed under terms of the MIT license
"""
#====================================================================================
#
#          FILE: ezqsub (python 3)
#        AUTHOR: Michael D Dacre, mike.dacre@gmail.com
#  ORGANIZATION: Stanford University
#       LICENSE: MIT License
#       VERSION: 0.1
#       CREATED: 2013-12-26 17:37
# Last modified: 2013-12-27 16:23
#
#   DESCRIPTION: Take a file of scripts and submit it to the gordon cluster
#                The file should be one line per job, the lines can be arbitrarily
#                long and complex bash scripts, just use semi-colons instead of new-
#                lines.
#                To modify the qsub parameters, use the -l command, to modify the
#                queue, use the -q command.  Note that the '-l nodes=' qsub Command
#                will always be '-l nodes=1:native' in this script.  To use multiple
#                nodes together with MPI, use the bundler.py script.
#
#         USAGE: ezqsub script_file.txt or ezqsub < script_file.txt
#
#====================================================================================
"""

import sys

# Set the defaults
default_address  = 'sua131'
default_queue    = 'normal'
default_params   = 'walltime=336:00:0'

default_threads  = 16
default_commands = 16

def run_parallel(infile, threads=default_threads, verbose=False, logfile=''):
    """Take a file path to a file of commands and execute them in parallel.
       threads variable takes the number of threads to run at once

       Returns a dictionary of results:
           command -> (output_code, stdout_and_stderr)

       If verbose, results of any failed command will also be logged
       to logfile (default STDERR)"""

    # We use the multi-threading pool and subprocess for this
    from subprocess import getstatusoutput as call
    from multiprocessing import Pool
    pool = Pool(processes=int(threads))

    # List to hold results
    threads = []

    # Open file, and for each line, split line into list by whitespace
    # and pass to subprocess via multiprocessing pool
    with open(infile) as file:
        for line in file:
            if not line.startswith('#'):
                command = [line.rstrip()]
                threads.append( (line.rstrip(), pool.apply_async(call, command)) )

    # Run the threads
    results = []
    for name, thread in threads:
        results.append( (name, thread.get()) )

    # Return results
    return results


def split(infile, commands=default_commands):
    """
    Take a file of scripts and split it into temp script files to be submitted
    to the cluster.
    The file should be one line per job, the lines can be arbitrarily
    long and complex bash scripts, just use semi-colons instead of new-
    lines.
    The commands variable specifies how many lines to create per file, or in 
    other words, how many jobs to run per node.
    NOTE: If the number of commands is less than the number of processes, the
    extras will be run in series.

    The first argument must be an open file handle
    """
    from tempfile import mkstemp
    import os
    
    outfile_name    = ''
    lines = 0
    files = []

    for line in infile:
        if not lines:
            lines = commands
            outfile_name = mkstemp(dir='/tmp', text=True)[1]
            files.append(outfile_name)

        with open(outfile_name, 'a') as outfile:
            print(line.rstrip(), file=outfile)

        lines = lines - 1

    return(files)

def pbs_submit(command, name='', template='', queue='normal', params=''):
    """ Take a command and an optional template and submit it to PBS.
        You can modify the queue with the queue variable or any other qsub
        parameters using the params variable """

    if not template:
        template = """#!/bin/bash
#PBS -S /bin/bash
"""
    if not name:
        name = command

    template = '\n'.join([template, ' '.join(["#PBS -N", name]), "cd $PBS_O_WORKDIR", ''])

    # Open pbs session
    pbs_command = subprocess.Popen('qsub', stdin=subprocess.PIPE, stdout=subprocess.PIPE)

    # Submit command
    pbs_command.stdin.write('\n'.join([template, command]).encode())
    pbs_command.stdin.close()

    # Get job number
    output = pbs_command.stdout.read().decode().rstrip()
    pbs_command.stdout.close()

    return output

def _get_args():
    """Command Line Argument Parsing"""
    import argparse, sys

    parser = argparse.ArgumentParser(
                 description=__doc__,
                 formatter_class=argparse.RawDescriptionHelpFormatter)

    # Optional Files
    parser.add_argument('-i', '--infile', nargs='?', default=sys.stdin,
                        help="Input file, Default STDIN")

    # Optional Arguments
    parser.add_argument('-q', '--queue', default=default_queue,
            help=''.join(["Queue Choice, Default: ", default_queue]) )
    parser.add_argument('-l', '--params', default=default_params,
            help=''.join([ "qsub parameters, Default: ", default_params]) )
    parser.add_argument('-a', '--billing', default=default_address,
            help=''.join(["Choose the address to bill to, find this with ",
                          "show_account or on portal.xsedeq.org",
                          ", Default: ", default_address]) )

    return parser

# Main function for direct running
def main():
    """Run directly"""
    # Get commandline arguments
    parser = _get_args()
    args = parser.parse_args()

    # Do Stuff
    if isinstance(args.infile, str):
        infile_string = True
    else:
        infile_string = False

    if infile_string:
        with open(args.infile, 'r') as infile:
            split_and_submit(infile, args.queue, args.params)
    else:
        split_and_submit(infile, args.queue, args.params)


# The end
if __name__ == '__main__':
    #print(run_parallel('t.txt'))
    with open('t.txt') as infile:
        print(split(infile))
    #main()
